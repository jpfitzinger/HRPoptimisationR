---
#title: "Testing the performance of Hierarchical Risk Parity for portfolio optimisation using JSE shares"
#author: "Johann Pfitzinger"
#date: "`r format(Sys.time(), '%B %e, %Y')`"
output: 
  bookdown::html_document2:
    number_sections: false
bibliography: Tex/ref.bib
link-citations: true
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, error = FALSE, warning = FALSE, message = FALSE, results = "asis")
knitr::opts_chunk$set(out.extra = 'style="display:block; margin:auto;"')
```
```{r plots}
load("./Data/Import.RData")
pacman::p_load(dplyr, quadprog, zoo, reshape2, ggplot2, corrplot, quantmod, tawny, gridExtra, stringr)
```

An important and well-acknowledged drawback of mean-variance portfolio optimization is its high degree of sample sensitivity. While the method should produce the lowest portfolio volatility _in-sample_, evidence suggests inferior _out-of-sample_ performance and diversification [@lopez_de_prado_building_2016]. This poses a serious challenge to the usefulness of practical applications of the method. Various approaches have been proposed to improve portfolio composition, including, amongst others, imposing minimum diversification constraints on the portfolio, or incorporating return priors (Black-Litterman). In a recent paper, @lopez_de_prado_building_2016 introduces an innovative new approach to the problem, by allocating portfolio weights based on an hierarchical clustering of the covariance matrix.

In the following analysis, I apply the Hierarchical Risk Parity (HRP) technique to a small sample of 20 JSE shares (Jan 2007 - Jun 2017), and compare portfolio performance and diversification to more standard alternative optimization methods. The outcome demonstrates a significant reduction in out-of-sample volatility, with the HRP portfolio achieving the highest sharpe ratio, as well as superior return and diversification.

### Clustered Covariance Matrix

The initial step in applying the HRP method, is to generate a clustered covariance matrix. This requires rearranging the covariance matrix such that the largest values lie along its diagonal and least correlated return series are furthest apart. The following figure displays the clustered correlation matrix for our sample of shares:

```{r corr, fig.cap="Clustered correlation matrix", fig.align='center', fig.width=4, fig.height=4}
corrplot(cor(prices.data), order="hclust", hclust.method = "single", addrect=3, tl.cex = 0.5, tl.col = "black")
#cluster <- cor(prices.data) %>% correlDist %>% dist %>% hclust("single")

#multiplot(corplot, cluster, layout = matrix(c(1,2), nrow=1, byrow=TRUE), wid = 10, hei = 10)
```

Capital is now distributed equally along the cluster hierarchy, which ensures a diversified allocation without the need to invert the covariance matrix, and thus better numerical stability. The R code to perform the hierarchical allocation is adapted from a Python notebook, which can be found here. The HRP code is printed at the end of this article.

### Portfolio backtesting {-}

```{r}
p <- optFx(rets, calc_int = 252, rebal_per = "months", methods = c("HRP","MVO","minVar","invVar","Equal"))
```


Using our sample of 20 JSE shares, I compute covariance statistics and portfolio allocations using trailing 12-month data and monthly rebalancing. All the results thus represent out-of-sample outcomes, computed in a rolling window over the sample period. I compare HRP to standard Mean-Variance optimisation (MVO) (using quadratic programming), as well as minimum variance, inverse variance, and equally weighted portfolios. Figure \@ref(fig:cret1) displays the cumulative return for the various methods, with the HRP outcome represented by the blue line: 

```{r cret1, fig.cap="Cumulative log returns", fig.align='center', fig.width=10}
l.rets <- p$port_returns %>% log1p %>% cumsum %>% melt
      l.rets$date <- rownames(p$port_returns) %>% as.Date %>% rep(ncol(p$port_returns))
      l.rets$label <- sapply(as.character(l.rets$variable), function(x) opt_methods[[x]][1])
      
      plot.cret <- ggplot(data = l.rets, aes(x = date)) + 
        geom_line(aes(y = value, color = label)) +
        scale_color_brewer(palette = "Set1", name = NULL) +
        labs(x = NULL, y = "Cumulative Log Returns") + 
        scale_y_continuous(breaks = seq(-2,2,0.25)) +
        theme_bw() + theme(legend.position = "top")
      
      p.hhi <- apply((p$weights*100)^2, c(1,3), sum) %>% melt
      p.hhi$date <- p.hhi$Var2 %>% as.Date
      p.hhi$label <- sapply(as.character(p.hhi$Var1), function(x) opt_methods[[x]][1])
      
      plot.hhi <- ggplot(data = p.hhi, aes(x = date)) + 
        geom_line(aes(y = value, color = label)) +
        scale_color_brewer(palette = "Set1", name = NULL) +
        labs(x = NULL, y = "Diversification (HHI on weights)") + 
        scale_y_continuous(breaks = seq(0,10000,1000)) +
        theme_bw() + theme(legend.position = "top")
  
      plot.cret
```

Visual inspection demonstrates the significant volatility reduction achieved by HRP over standard Mean-Variance optimization. The following plots show the sharpe ratios and volatility of the various methods:

```{r srvol1, fig.cap="Sharpe Ratio and Portfolio Volatility", fig.align='center', fig.width=10, fig.height=4}
sr <- colMeans(p$port_returns)*252 / (apply(p$port_returns,2,sd)*sqrt(252))
      plot.sr <- ggplot() + geom_col(aes(str_wrap(sapply(as.character(names(sr)), function(x) opt_methods[[x]][1]), width = 15), sr, fill = names(sr))) +
        labs(x = NULL, y = "Sharpe Ratio") +
        scale_fill_brewer(palette = "Set1") +
        theme_bw() + theme(legend.position = "none")
      
      vol <- apply(p$port_returns,2,sd)*sqrt(252)
      plot.vol <- ggplot() + geom_col(aes(str_wrap(sapply(as.character(names(vol)), function(x) opt_methods[[x]][1]), width = 15), vol, fill = names(vol))) +
        labs(x = NULL, y = "Portfolio Volatility") +
        scale_fill_brewer(palette = "Set1") +
        theme_bw() + theme(legend.position = "none")
 
      multiplot(plot.sr, plot.vol, cols = 2, wid = 100, hei = 100)
```

The outcome echoes the findings of @lopez_de_prado_building_2016, lending support to the HRP method --- particularly when compared to standard MVO. The diversification benefit can be demonstrated by calculating the average HHI for the portfolios over the sample period, with HRP diversification at `r round(mean(p.hhi[p.hhi$Var1=="HRP",]$value))/100`, compared to MVO at `r round(mean(p.hhi[p.hhi$Var1=="MVO",]$value))/100` (scaled from 0 - 100). The poor diversification of MVO is reflective of this method's tendency to over-allocate to low-volatility assets. An interesting follow-on question is the performance of HRP in comparison to a standard MVO portfolio with a maximum allocation constraint of 20%. By limiting the asset weights to 20%, a more diversified MVO allocation is forced.

### HRP vs. weight-constrained MVO {-}

```{r}
max.w <- 0.2
p <- optFx(rets, calc_int = 252, rebal_per = "months", methods = c("HRP","cHRP","MVO","cMVO"))
```

Figure \@ref(fig:cret2) displays the cumulative return of the HRP and MVO portfolios as before, but adding an MVO portfolio with a maximum allocation constraint of 20%:

```{r cret2, fig.cap="Cumulative log returns", fig.align='center', fig.width=10}
l.rets <- p$port_returns %>% log1p %>% cumsum %>% melt
      l.rets$date <- rownames(p$port_returns) %>% as.Date %>% rep(ncol(p$port_returns))
      l.rets$label <- sapply(as.character(l.rets$variable), function(x) opt_methods[[x]][1])
      
      plot.cret <- ggplot(data = l.rets, aes(x = date)) + 
        geom_line(aes(y = value, color = label)) +
        scale_color_brewer(palette = "Set1", name = NULL) +
        labs(x = NULL, y = "Cumulative Log Returns") + 
        scale_y_continuous(breaks = seq(-2,2,0.25)) +
        theme_bw() + theme(legend.position = "top")
  
      plot.cret
```

```{r srvol2, fig.cap="Sharpe Ratio and Portfolio Volatility", fig.align='center', fig.width=10, fig.height=4}
sr <- colMeans(p$port_returns)*252 / (apply(p$port_returns,2,sd)*sqrt(252))
      plot.sr <- ggplot() + geom_col(aes(str_wrap(sapply(as.character(names(sr)), function(x) opt_methods[[x]][1]), width = 15), sr, fill = names(sr))) +
        labs(x = NULL, y = "Sharpe Ratio") +
        scale_fill_brewer(palette = "Set1") +
        theme_bw() + theme(legend.position = "none")
      
      vol <- apply(p$port_returns,2,sd)*sqrt(252)
      plot.vol <- ggplot() + geom_col(aes(str_wrap(sapply(as.character(names(vol)), function(x) opt_methods[[x]][1]), width = 15), vol, fill = names(vol))) +
        labs(x = NULL, y = "Portfolio Volatility") +
        scale_fill_brewer(palette = "Set1") +
        theme_bw() + theme(legend.position = "none")
 
      multiplot(plot.sr, plot.vol, cols = 2, wid = 100, hei = 100)
```

While the HRP portfolio still achieves the lowest volatility, the constrained MVO portfolio now out-performs it, both in terms of sharpe ratio, as well as return. This result is interesting, and demonstrates that while HRP is successful in addressing the concerns around starndard MVO, it is by no means the only feasible approach.

### Conclusion {-}

This article has outlined a novel approach to portfolio  optimisation based on an hierarchical clustering of the covariance matrix. Using a small sample of JSE shares and performing portfolio back-testing over a 10 year period demonstrates the potential significant improvement of the method over standard portfolio optmisation methods. HRP achieves the highest sharpe ratio among a set of 5 alternative techniques. Comparing the method to a weight-constrained MVO portfolio shows that the method is however not  the only solution to the MVO diversification problem.

In future analysis, I plan to perform a more rigorous comparison of these methods over various samples of assets stratified by their correlation characteristics. While these results are promising and echo the findings of other authors, broader testing is required before any definitive claims can be made.

### Code Appendix {-}

```{r, echo = T}
#--------------------------------------------------------------------------
# Hierarchical Risk Parity (by LdP)
#--------------------------------------------------------------------------

      getHRP <- function(cov, corr, max = NULL, min = NULL, return_raw = NULL, robust_cov = F) {
        # Construct a hierarchical portfolio
        if (robust_cov == T) {
          cov <- cov_shrink(return_raw)
          corr <- cov2cor(cov)
        }
        
        # Set the constraint matrix
        if (is.null(max)) max <- rep(1,ncol(cov))
        if (is.null(min)) min <- rep(0,ncol(cov))
        if (length(max)==1) max <- rep(max,ncol(cov)) else if (length(max)<ncol(cov)) stop("Provide correct weights")
        if (length(min)==1) min <- rep(min,ncol(cov)) else if (length(min)<ncol(cov)) stop("Provide correct weights")
        const <- rbind(max, min)
        
        # check constraints
        if (sum(const[1,]) < 1 | sum(const[2,]) > 1) stop("Incompatible weights")
        
        distmat <- ((1 - corr) / 2)^0.5
        clustOrder <- hclust(dist(distmat), method = 'single')$order
        out <- getRecBipart(cov, clustOrder, const)
        return(out)
      }
      
      getClusterVar <- function(cov, cItems) {
        # compute cluster variance from the inverse variance portfolio above
        covSlice <- cov[cItems, cItems]
        weights <- getIVP(covSlice)
        cVar <- t(weights) %*% as.matrix(covSlice) %*% weights
        return(cVar)
      }
      
      getRecBipart <- function(cov, sortIx, const) {
        
        w <- rep(1, ncol(cov))
        
        # create recursion function within parent function to avoid use of globalenv
        recurFun <- function(cov, sortIx, const) {
          # get first half of sortIx which is a cluster order
          subIdx <- 1:trunc(length(sortIx)/2)
          
          # subdivide ordering into first half and second half
          cItems0 <- sortIx[subIdx]
          cItems1 <- sortIx[-subIdx]
          
          # compute cluster variances of covariance matrices indexed
          # on first half and second half of ordering
          cVar0 <- getClusterVar(cov, cItems0)
          cVar1 <- getClusterVar(cov, cItems1)
          alpha <- 1 - cVar0/(cVar0 + cVar1)
          
          # determining whether weight constraint binds
          alpha <- min(sum(const[1,cItems0]) / w[cItems0[1]],
                       max(sum(const[2,cItems0]) / w[cItems0[1]], 
                           alpha))
          alpha <- 1 - min(sum(const[1,cItems1]) / w[cItems1[1]], 
                           max(sum(const[2,cItems1]) / w[cItems1[1]], 
                               1 - alpha))
          
          w[cItems0] <<- w[cItems0] * rep(alpha, length(cItems0))
          w[cItems1] <<- w[cItems1] * rep((1-alpha), length(cItems1))
          
          # rerun the function on a half if the length of that half is greater than 1
          if(length(cItems0) > 1) {
            recurFun(cov, cItems0, const)
          }
          if(length(cItems1) > 1) {
            recurFun(cov, cItems1, const)
          }
          
        }
        
        # run recursion function
        recurFun(cov, sortIx, const)
        return(w)
      }
      
```

### References {-}






